Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1 (use --cores to define parallelism)
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	summary
	1

[Tue Oct 27 10:55:27 2020]
rule summary:
    input: 10000_1_analysis.txt, 10000_100_analysis.txt, 10000_5_analysis.txt, 10000_10_analysis.txt, 10000_30_analysis.txt, 1000_1_analysis.txt, 1000_100_analysis.txt, 1000_5_analysis.txt, 1000_10_analysis.txt, 1000_30_analysis.txt, 3162_1_analysis.txt, 3162_100_analysis.txt, 3162_5_analysis.txt, 3162_10_analysis.txt, 3162_30_analysis.txt, 31622_1_analysis.txt, 31622_100_analysis.txt, 31622_5_analysis.txt, 31622_10_analysis.txt, 31622_30_analysis.txt
    output: summary.csv
    jobid: 0

/home/evarol/.local/lib/python3.7/site-packages/snakemake/workflow.py:106: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version
of pandas will change to not sort by default.

To accept the future behavior, pass 'sort=False'.

To retain the current behavior and silence the warning, pass 'sort=True'.

  run_local=True,
[Tue Oct 27 10:55:27 2020]
Finished job 0.
1 of 1 steps (100%) done
